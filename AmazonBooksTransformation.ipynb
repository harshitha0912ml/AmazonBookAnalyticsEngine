{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af2d57c9-afef-4c30-91f0-4e31c75c685e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##wite ur config here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4f28a3a-a911-48d9-afc1-8ce16e9ffe30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[FileInfo(path='abfss://booksbronze@amazonbooks0912.dfs.core.windows.net/amazon_books (1).csv', name='amazon_books (1).csv', size=52627, modificationTime=1752388496000)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbutils.fs.ls(\"abfss://booksbronze@amazonbooks0912.dfs.core.windows.net/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b84fed0-6335-473a-8ebd-d0dc01d8eff5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame Schema:\nroot\n |-- author: string (nullable = true)\n |-- genre: string (nullable = true)\n |-- image_url: string (nullable = true)\n |-- price: string (nullable = true)\n |-- rating: double (nullable = true)\n |-- reviews_count: string (nullable = true)\n |-- title: string (nullable = true)\n\n\nTotal rows: 305\nTotal columns: 7\n\nFirst 5 rows:\n+----------------+---------+--------------------+-----+------+-------------+--------------------+\n|          author|    genre|           image_url|price|rating|reviews_count|               title|\n+----------------+---------+--------------------+-----+------+-------------+--------------------+\n|   Joseph Nguyen|Self Help|https://m.media-a...|  163|   4.5|       11,964|Don't Believe Eve...|\n|     Cal Newport|Self Help|https://m.media-a...|  232|   4.5|       30,704|DEEP WORK: RULES ...|\n|Thibaut Meurisse|Self Help|https://m.media-a...|  181|   4.4|       14,851|Dopamine Detox: A...|\n|   Brianna Wiest|Self Help|https://m.media-a...|  259|   4.4|       23,042|The Mountain Is Y...|\n|     Jeff Keller|Self Help|https://m.media-a...|  135|   4.5|       26,132|Attitude is Every...|\n+----------------+---------+--------------------+-----+------+-------------+--------------------+\nonly showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Load CSV file into a Spark DataFrame\n",
    "file_path = \"abfss://booksbronze@amazonbooks0912.dfs.core.windows.net/amazon_books (1).csv\"\n",
    "\n",
    "# Method 1: Using spark.read.csv (recommended for Databricks)\n",
    "df = spark.read.csv(\n",
    "    file_path,\n",
    "    header=True,        # First row contains column names\n",
    "    inferSchema=True,   # Automatically infer data types\n",
    "    escape='\"'          # Handle escaped quotes in data\n",
    ")\n",
    "\n",
    "# Display basic info about the DataFrame\n",
    "print(\"DataFrame Schema:\")\n",
    "df.printSchema()\n",
    "\n",
    "print(f\"\\nTotal rows: {df.count()}\")\n",
    "print(f\"Total columns: {len(df.columns)}\")\n",
    "\n",
    "# Show first few rows\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd00d9c8-d904-46fb-9449-e970c45c18f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Book Dataset Cleaning Script for Databricks\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a62b71fb-a559-41d8-820f-d3c781c1a04e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Clean author names\n",
    "\n",
    "books_df = df.withColumn(\n",
    "    \"author\",\n",
    "    when(col(\"author\").rlike(\"^by$|^by \"), regexp_replace(col(\"author\"), \"^by\\\\s*\", \"\"))\n",
    "    .when(col(\"author\").contains(\",\"), \n",
    "          regexp_replace(col(\"author\"), \",$\", \"\"))  # Remove trailing comma\n",
    "    .otherwise(col(\"author\"))\n",
    ").withColumn(\n",
    "    \"author\",\n",
    "    when(col(\"author\") == \"by\", lit(\"Unknown\"))\n",
    "    .when(col(\"author\").isNull() | (col(\"author\") == \"\"), lit(\"Unknown\"))\n",
    "    .otherwise(trim(col(\"author\")))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cd5df11-2125-4c59-a7de-7bac9ffef78b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#cleaning genres\n",
    "books_df = books_df.withColumn(\n",
    "    \"genre\",\n",
    "    when(col(\"genre\").isNull() | (col(\"genre\") == \"\"), lit(\"Unknown\"))\n",
    "    .otherwise(initcap(trim(col(\"genre\"))))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5eb5eb4-ed63-486f-a207-d0170e39938c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#cleaning price\n",
    "books_df = books_df.withColumn(\n",
    "    \"price_numeric\",\n",
    "    when(col(\"price\").isNull() | (col(\"price\") == \"\") | (col(\"price\") == \"0\"), lit(0.0))\n",
    "    .otherwise(\n",
    "        regexp_replace(col(\"price\"), \"[^0-9.]\", \"\").cast(\"double\")\n",
    "    )\n",
    ").drop(\"price\").withColumnRenamed(\"price_numeric\", \"price\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9462e016-29b7-47a4-97d1-4c3a4312e2c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "books_df = books_df.withColumn(\n",
    "    \"reviews_count_numeric\",\n",
    "    when(col(\"reviews_count\").isNull() | (col(\"reviews_count\") == \"\"), lit(0))\n",
    "    .otherwise(\n",
    "        regexp_replace(col(\"reviews_count\"), \"[^0-9]\", \"\").cast(\"integer\")\n",
    "    )\n",
    ").drop(\"reviews_count\").withColumnRenamed(\"reviews_count_numeric\", \"reviews_count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd6b7b6d-11fe-4d20-8b0e-cd1d44e8f13d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 5. Handle rating column - ensure it's within valid range\n",
    "\n",
    "books_df = books_df.withColumn(\n",
    "    \"rating\",\n",
    "    when(col(\"rating\").isNull() | (col(\"rating\") < 0) | (col(\"rating\") > 5), lit(0.0))\n",
    "    .otherwise(col(\"rating\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79f88b95-58f6-48c4-bd9f-0238f010d67c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 6. Clean title column\n",
    "\n",
    "books_df = books_df.withColumn(\n",
    "    \"title\",\n",
    "    when(col(\"title\").isNull() | (col(\"title\") == \"\"), lit(\"Unknown Title\"))\n",
    "    .otherwise(\n",
    "        # Remove HTML entities and clean up encoding issues\n",
    "        regexp_replace(\n",
    "            regexp_replace(col(\"title\"), \"â€™\", \"'\"),\n",
    "            \"Ç€\", \"\"\n",
    "        )\n",
    "    )\n",
    ").withColumn(\n",
    "    \"title\",\n",
    "    trim(col(\"title\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49e89a2b-9d84-4254-b15e-da1df67d0191",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8. Removing duplicates...\n"
     ]
    }
   ],
   "source": [
    "print(\"8. Removing duplicates...\")\n",
    "books_df = books_df.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d1bf62e-e6e4-4211-95b4-41d0f9938311",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# 9. Create additional derived columns for analysis\n",
    "\n",
    "books_df = books_df.withColumn(\n",
    "    \"price_category\",\n",
    "    when(col(\"price\") == 0, \"Free\")\n",
    "    .when(col(\"price\") <= 150, \"Low\")\n",
    "    .when(col(\"price\") <= 300, \"Medium\")\n",
    "    .otherwise(\"High\")\n",
    ").withColumn(\n",
    "    \"rating_category\",\n",
    "    when(col(\"rating\") == 0, \"No Rating\")\n",
    "    .when(col(\"rating\") >= 4.5, \"Excellent\")\n",
    "    .when(col(\"rating\") >= 4.0, \"Good\")\n",
    "    .when(col(\"rating\") >= 3.5, \"Average\")\n",
    "    .otherwise(\"Below Average\")\n",
    ").withColumn(\n",
    "    \"popularity_score\",\n",
    "    when(col(\"reviews_count\") == 0, 0)\n",
    "    .otherwise(col(\"rating\") * log10(col(\"reviews_count\") + 1))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9ae2414-2fed-4767-abbd-78d2fc02d4df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nNull value counts after cleaning:\nauthor: 0\ngenre: 0\nimage_url: 0\nrating: 0\ntitle: 0\nprice: 0\nreviews_count: 0\nprice_category: 0\nrating_category: 0\npopularity_score: 0\n"
     ]
    }
   ],
   "source": [
    "# Check for null values\n",
    "print(\"\\nNull value counts after cleaning:\")\n",
    "for column in books_df.columns:\n",
    "    null_count = books_df.filter(col(column).isNull()).count()\n",
    "    print(f\"{column}: {null_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9dfe6335-1346-4d3a-a958-8d6b738ced17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving to: abfss://bookssilver@amazonbooks0912.dfs.core.windows.net/cleaned_books_data\n✅ Successfully saved cleaned data as Parquet format\nSaving CSV to: abfss://bookssilver@amazonbooks0912.dfs.core.windows.net/cleaned_books_data_csv\n✅ Successfully saved cleaned data as CSV format\n"
     ]
    }
   ],
   "source": [
    "#save to silver layer here\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "AmazonBooksTransformation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}